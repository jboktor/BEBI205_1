{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from skimage import exposure, util\n",
    "from skimage.transform import resize\n",
    "import sklearn.model_selection\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Activation, BatchNormalization, Conv2D, MaxPool2D, Softmax\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = glob.glob( '../data/input/keren/*.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Background', 1: 'Unidentified', 2: 'Endothelial', 3: 'Mesenchyme', 4: 'Tumor', 5: 'Tregs', 6: 'CD4T', 7: 'CD8T', 8: 'CD3T', 9: 'Nk', 10: 'Bcell', 11: 'Neutrophil', 12: 'Macrophage', 13: 'DC', 14: 'DC_Mono', 15: 'Mono_Neu', 16: 'Immune_other', 17: 'FAILED_HARMONIZATION'}\n",
      "{0: 'Au', 1: 'B7H3', 2: 'Beta-catenin', 3: 'C', 4: 'CD11b', 5: 'CD11c', 6: 'CD138', 7: 'CD16', 8: 'CD163', 9: 'CD20', 10: 'CD209', 11: 'CD3', 12: 'CD31', 13: 'CD4', 14: 'CD45', 15: 'CD45RO', 16: 'CD56', 17: 'CD63', 18: 'CD68', 19: 'CD8', 20: 'CSF-1R', 21: 'Ca', 22: 'EGFR', 23: 'Fe', 24: 'FoxP3', 25: 'H3K27me3', 26: 'H3K9ac', 27: 'HLA-DR', 28: 'HLA-Class-1', 29: 'IDO', 30: 'Keratin17', 31: 'Keratin6', 32: 'Ki67', 33: 'Lag3', 34: 'MPO', 35: 'Na', 36: 'OX40', 37: 'P', 38: 'PDL1', 39: 'PD1', 40: 'PanCK', 41: 'SMA', 42: 'Si', 43: 'Ta', 44: 'Vimentin', 45: 'dsDNA', 46: 'p53', 47: 'pS6', 48: 'background', 49: 'sum of H3K27me3, H3K9ac, and dsDNA', 50: 'sum_PanCK_CD45'}\n"
     ]
    }
   ],
   "source": [
    "# Defining cell type and channel dictionaries based off meta.yaml\n",
    "with open(\"../data/input/keren/meta.yaml\", 'r') as stream:\n",
    "    data = yaml.safe_load(stream)\n",
    "\n",
    "cell_types_dict = data['cell_types']\n",
    "channels_dict = {i: j for i, j in enumerate(data['channels'])}\n",
    "print(cell_types_dict)\n",
    "print(channels_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2048, 2048, 51) (1, 2048, 2048, 2)\n"
     ]
    }
   ],
   "source": [
    "with np.load(data_paths[1], allow_pickle=True) as f:\n",
    "    X = f['X']\n",
    "    y = f['y']\n",
    "    cell_types = f['cell_types'].item()\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markers identified in previous notebook \n",
    "variable_markers = pd.read_pickle('../data/interim/variable_markers_df.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "    \"\"\"\n",
    "    Normalizes an image using contrast stretching.\n",
    "    \"\"\"\n",
    "    # stretch contrast\n",
    "    p2, p98 = np.percentile(image, (2, 98))\n",
    "    image = exposure.rescale_intensity(image, in_range=(p2, p98))\n",
    "    # rescale the image to the range [-1, 1]\n",
    "    image = util.img_as_float32(image)\n",
    "    return image\n",
    "def select_random_cell_indices(cell_types, n=25):\n",
    "    \"\"\"\n",
    "    Returns a list of cell indices containing 25 cells of each type.\n",
    "    \"\"\"\n",
    "    random.seed(42)\n",
    "    selected_cell_indices = []\n",
    "    for value in set(cell_types.values()):\n",
    "        keys = [key for key, val in cell_types.items() if val == value]\n",
    "        np.random.shuffle(keys)\n",
    "        selected_cell_indices.extend(keys[:n])\n",
    "    return selected_cell_indices\n",
    "def get_cell_views(image, cell_mask, cell_types):\n",
    "    \"\"\"\n",
    "    Returns a list of cell views and a list of associated cell types.\n",
    "    \"\"\"\n",
    "    cell_views = []\n",
    "    cell_types_list = []\n",
    "    # get random indices for 15 cells of each type cell type\n",
    "    cell_type_indices = select_random_cell_indices(cell_types, n=15)\n",
    "    for j in cell_type_indices:\n",
    "        # mask the cell view to get only the current cell type\n",
    "        masked_view = np.ma.masked_where(cell_mask != j, image)\n",
    "        # append the masked view and associated cell type to the lists\n",
    "        cell_views.append(masked_view)\n",
    "        cell_types_list.append(cell_types[j])\n",
    "    return cell_views, cell_types_list\n",
    "def get_cell_boxes(image, cell_mask, cell_types, use_all_cells=False):\n",
    "    \"\"\"\n",
    "    Returns a list of bounding boxes and a list of associated cell types.\n",
    "    \"\"\"\n",
    "    cell_boxes = []\n",
    "    cell_types_list = []\n",
    "    if use_all_cells:\n",
    "        cell_type_indices = list(np.unique(cell_mask))\n",
    "    else:        \n",
    "        # get random indices for 15 cells of each type cell type\n",
    "        cell_type_indices = select_random_cell_indices(cell_types, n=30)\n",
    "    # loop through cell types\n",
    "    for j in cell_type_indices:\n",
    "        cell_type = cell_types[j]\n",
    "        # get indices for cells of current cell type\n",
    "        cell_indices = np.where(cell_mask == j)\n",
    "        # calculate cell center coordinates\n",
    "        cell_centers = np.stack([cell_indices[0], cell_indices[1]], axis=-1)\n",
    "        cell_centers = cell_centers.mean(axis=0)\n",
    "        cell_center_row, cell_center_col = cell_centers.astype(int)\n",
    "        # calculate cell bounding box coordinates\n",
    "        bbox_row_min = max(0, cell_center_row - 10)\n",
    "        bbox_row_max = min(cell_mask.shape[0]-1, cell_center_row + 10)\n",
    "        bbox_col_min = max(0, cell_center_col - 10)\n",
    "        bbox_col_max = min(cell_mask.shape[1]-1, cell_center_col + 10)\n",
    "        # slice image to get cell view\n",
    "        cell_view = image[bbox_row_min:bbox_row_max+1, bbox_col_min:bbox_col_max+1]\n",
    "        # resize cell view to desired shape\n",
    "        cell_view = resize(cell_view, (21, 21))\n",
    "        # append the cell view and associated cell type to the lists\n",
    "        cell_boxes.append(cell_view)\n",
    "        cell_types_list.append(cell_type)\n",
    "    return cell_boxes, cell_types_list\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Image intensity using contrast stretching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:10<00:00,  4.66it/s]\n"
     ]
    }
   ],
   "source": [
    "X_norm = np.zeros_like(X)\n",
    "for i in tqdm(range(X.shape[-1])):\n",
    "    X_norm[..., i] = normalize_image(X[..., i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create cell views using bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [02:53<00:00,  5.99s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create empty arrays with the desired shape and dtype\n",
    "cell_views = np.empty((len(variable_markers.index),), dtype=object)\n",
    "cell_types_list = np.empty((len(variable_markers.index),), dtype=object)\n",
    "\n",
    "for i, channel_key in enumerate(tqdm(variable_markers.index)):\n",
    "    # print('Processing: ' + channels_dict[channel_key])\n",
    "    cell_views[i], cell_types_list[i] = get_cell_boxes(X_norm[0,:,:,channel_key], y[0,:,:,0], cell_types)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format cell views for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_cell_views = np.empty((len(cell_views), len(cell_views[0]), 21, 21))\n",
    "for i, channel in enumerate(cell_views):\n",
    "    reshaped_cell_views[i] = np.stack(channel)\n",
    "cell_views_transposed = np.transpose(reshaped_cell_views, (1, 2, 3, 0))\n",
    "\n",
    "reshaped_cell_types = np.empty((len(cell_types_list), len(cell_types_list[0])))\n",
    "for i, channel in enumerate(cell_types_list):\n",
    "    reshaped_cell_types[i] = np.stack(channel)\n",
    "reshaped_cell_types = reshaped_cell_types.transpose((1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(443, 21, 21, 29) (443,)\n"
     ]
    }
   ],
   "source": [
    "print(cell_views_transposed.shape, reshaped_cell_types[:,0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = cell_views_transposed.shape[1]\n",
    "IMG_WIDTH = cell_views_transposed.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "# Create dataset builder\n",
    "class DatasetBuilder(object):\n",
    "    def __init__(self,\n",
    "                 X,\n",
    "                 y,\n",
    "                 batch_size=1,\n",
    "                 rotation_range=180,\n",
    "                 scale_range=(0.75, 1.25)):\n",
    "        self.X = X\n",
    "        self.y = tf.keras.utils.to_categorical(y)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.rotation_range = float(rotation_range)\n",
    "        self.scale_range = scale_range\n",
    "        \n",
    "        # Create dataset\n",
    "        self._create_dataset()\n",
    "        \n",
    "    def _augment(self, *args):\n",
    "        img = args[0]\n",
    "        label = args[1]\n",
    "        \n",
    "        theta = tf.random.uniform([1], 0, 2*np.pi*self.rotation_range/360)\n",
    "        img = tfa.image.rotate(img, theta)\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "        \n",
    "        return (img, label)\n",
    "        \n",
    "    def _create_dataset(self):\n",
    "        X_train, X_temp, y_train, y_temp = sklearn.model_selection.train_test_split(self.X, self.y, train_size=0.8)\n",
    "        X_val, X_test, y_val, y_test = sklearn.model_selection.train_test_split(X_temp, y_temp, train_size=0.5)\n",
    "        \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "        \n",
    "        self.train_dataset = train_dataset.shuffle(256).batch(self.batch_size).map(self._augment)\n",
    "        self.val_dataset = val_dataset.batch(self.batch_size)\n",
    "        self.test_dataset = test_dataset.batch(self.batch_size)\n",
    "\n",
    "\n",
    "# Define the convolutional neural network\n",
    "def create_conv_classifier():\n",
    "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, 29),\n",
    "                   name='conv_classifier_input')\n",
    "    x = Conv2D(64, (3,3), padding='SAME')(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPool2D(strides=(2,2))(x) # 16, 16\n",
    "    x = Conv2D(64, (3,3), padding='SAME')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPool2D(strides=(2,2))(x) # 8,8\n",
    "    x = Conv2D(64, (3,3), padding='SAME')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPool2D(strides=(2,2))(x) # 4,4\n",
    "    x = Conv2D(64, (3,3), padding='SAME')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = MaxPool2D(strides=(2,2))(x) # 2,2\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(18)(x)\n",
    "    x = Softmax(axis=-1)(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "conv_classifier = create_conv_classifier()\n",
    "# plot_model(conv_classifier, show_shapes=True)\n",
    "\n",
    "# Define the loss function\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy() \n",
    "# Define training algorithm\n",
    "conv_optimizer = tf.keras.optimizers.Adam(lr=1e-3, clipnorm=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DatasetBuilder(cell_views_transposed, reshaped_cell_types[:,0], batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "training_steps_per_epoch=512\n",
    "n_epochs=30\n",
    "\n",
    "# Define callbacks\n",
    "conv_model_path = '../data/models/conv'\n",
    "\n",
    "conv_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        conv_model_path, monitor='val_loss',\n",
    "        save_best_only=True, verbose=1,\n",
    "        save_weights_only=False)\n",
    "]\n",
    "\n",
    "conv_callbacks.append(\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, verbose=1,\n",
    "        patience=3, min_lr=1e-7)\n",
    ")\n",
    "\n",
    "# Define metrics\n",
    "recall_0 = tf.keras.metrics.Recall(class_id=0)\n",
    "recall_1 = tf.keras.metrics.Recall(class_id=1)\n",
    "\n",
    "precision_0 = tf.keras.metrics.Precision(class_id=0)\n",
    "precision_1 = tf.keras.metrics.Precision(class_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_classifier.compile(optimizer=conv_optimizer, \n",
    "                          loss=loss_function, \n",
    "                          metrics = [recall_0, recall_1, precision_0, precision_1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 15:31:21.722623: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [354,18]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-04-21 15:31:21.722956: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [354,18]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/6 [========================>.....] - ETA: 0s - loss: 3.5478 - recall_2: 0.0000e+00 - recall_3: 0.0000e+00 - precision_2: 0.0000e+00 - precision_3: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 15:31:24.170611: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [44,18]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2.84749, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 5s 498ms/step - loss: 3.4645 - recall_2: 0.0000e+00 - recall_3: 0.0000e+00 - precision_2: 0.0000e+00 - precision_3: 0.0000e+00 - val_loss: 2.8475 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 2.1566 - recall_2: 0.0000e+00 - recall_3: 0.2857 - precision_2: 0.0000e+00 - precision_3: 1.0000     \n",
      "Epoch 2: val_loss improved from 2.84749 to 2.83940, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 631ms/step - loss: 2.1357 - recall_2: 0.0000e+00 - recall_3: 0.2857 - precision_2: 0.0000e+00 - precision_3: 1.0000 - val_loss: 2.8394 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.5844 - recall_2: 0.0000e+00 - recall_3: 0.7692 - precision_2: 0.0000e+00 - precision_3: 0.9091\n",
      "Epoch 3: val_loss improved from 2.83940 to 2.80475, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 482ms/step - loss: 1.5815 - recall_2: 0.0000e+00 - recall_3: 0.7857 - precision_2: 0.0000e+00 - precision_3: 0.8462 - val_loss: 2.8048 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "6/6 [==============================] - ETA: 0s - loss: 1.2714 - recall_2: 0.0000e+00 - recall_3: 0.7143 - precision_2: 0.0000e+00 - precision_3: 1.0000    \n",
      "Epoch 4: val_loss improved from 2.80475 to 2.76655, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 465ms/step - loss: 1.2714 - recall_2: 0.0000e+00 - recall_3: 0.7143 - precision_2: 0.0000e+00 - precision_3: 1.0000 - val_loss: 2.7665 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 1.0523 - recall_2: 0.0000e+00 - recall_3: 0.6154 - precision_2: 0.0000e+00 - precision_3: 1.0000\n",
      "Epoch 5: val_loss improved from 2.76655 to 2.74703, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 506ms/step - loss: 1.0600 - recall_2: 0.0000e+00 - recall_3: 0.6429 - precision_2: 0.0000e+00 - precision_3: 1.0000 - val_loss: 2.7470 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.8452 - recall_2: 0.0000e+00 - recall_3: 1.0000 - precision_2: 0.0000e+00 - precision_3: 0.9091\n",
      "Epoch 6: val_loss improved from 2.74703 to 2.69534, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 439ms/step - loss: 0.8790 - recall_2: 0.0000e+00 - recall_3: 0.8571 - precision_2: 0.0000e+00 - precision_3: 0.9231 - val_loss: 2.6953 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.7356 - recall_2: 0.0000e+00 - recall_3: 1.0000 - precision_2: 0.0000e+00 - precision_3: 1.0000\n",
      "Epoch 7: val_loss improved from 2.69534 to 2.62141, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 440ms/step - loss: 0.7268 - recall_2: 0.0000e+00 - recall_3: 1.0000 - precision_2: 0.0000e+00 - precision_3: 1.0000 - val_loss: 2.6214 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6720 - recall_2: 0.0000e+00 - recall_3: 1.0000 - precision_2: 0.0000e+00 - precision_3: 1.0000\n",
      "Epoch 8: val_loss improved from 2.62141 to 2.55407, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 415ms/step - loss: 0.7004 - recall_2: 0.0000e+00 - recall_3: 0.9286 - precision_2: 0.0000e+00 - precision_3: 1.0000 - val_loss: 2.5541 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.5613 - recall_2: 0.0000e+00 - recall_3: 1.0000 - precision_2: 0.0000e+00 - precision_3: 0.9333\n",
      "Epoch 9: val_loss improved from 2.55407 to 2.54119, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 412ms/step - loss: 0.5613 - recall_2: 0.0000e+00 - recall_3: 1.0000 - precision_2: 0.0000e+00 - precision_3: 0.9333 - val_loss: 2.5412 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.5173 - recall_2: 0.0000e+00 - recall_3: 1.0000 - precision_2: 0.0000e+00 - precision_3: 1.0000\n",
      "Epoch 10: val_loss improved from 2.54119 to 2.48466, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 453ms/step - loss: 0.5286 - recall_2: 0.0000e+00 - recall_3: 1.0000 - precision_2: 0.0000e+00 - precision_3: 1.0000 - val_loss: 2.4847 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4735 - recall_2: 0.0000e+00 - recall_3: 1.0000 - precision_2: 0.0000e+00 - precision_3: 0.9286\n",
      "Epoch 11: val_loss improved from 2.48466 to 2.42609, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 399ms/step - loss: 0.4887 - recall_2: 0.0000e+00 - recall_3: 1.0000 - precision_2: 0.0000e+00 - precision_3: 0.9333 - val_loss: 2.4261 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4849 - recall_2: 0.0000e+00 - recall_3: 0.9231 - precision_2: 0.0000e+00 - precision_3: 1.0000\n",
      "Epoch 12: val_loss improved from 2.42609 to 2.36119, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 429ms/step - loss: 0.4724 - recall_2: 0.0000e+00 - recall_3: 0.9286 - precision_2: 0.0000e+00 - precision_3: 1.0000 - val_loss: 2.3612 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.4184 - recall_2: 0.0000e+00 - recall_3: 0.9091 - precision_2: 0.0000e+00 - precision_3: 0.9091     \n",
      "Epoch 13: val_loss improved from 2.36119 to 2.31967, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 460ms/step - loss: 0.4093 - recall_2: 0.0000e+00 - recall_3: 0.9286 - precision_2: 0.0000e+00 - precision_3: 0.9286 - val_loss: 2.3197 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3459 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000       \n",
      "Epoch 14: val_loss improved from 2.31967 to 2.25308, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 416ms/step - loss: 0.3438 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000 - val_loss: 2.2531 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3713 - recall_2: 0.0000e+00 - recall_3: 1.0000 - precision_2: 0.0000e+00 - precision_3: 0.9333\n",
      "Epoch 15: val_loss improved from 2.25308 to 2.15212, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 465ms/step - loss: 0.3794 - recall_2: 0.0000e+00 - recall_3: 1.0000 - precision_2: 0.0000e+00 - precision_3: 0.9333 - val_loss: 2.1521 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.3850 - recall_2: 0.0000e+00 - recall_3: 0.9286 - precision_2: 0.0000e+00 - precision_3: 1.0000\n",
      "Epoch 16: val_loss improved from 2.15212 to 2.11945, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 564ms/step - loss: 0.3850 - recall_2: 0.0000e+00 - recall_3: 0.9286 - precision_2: 0.0000e+00 - precision_3: 1.0000 - val_loss: 2.1194 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.2536 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000     \n",
      "Epoch 17: val_loss did not improve from 2.11945\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 0.2536 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000 - val_loss: 2.1860 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.3041 - recall_2: 0.0000e+00 - recall_3: 1.0000 - precision_2: 0.0000e+00 - precision_3: 1.0000\n",
      "Epoch 18: val_loss did not improve from 2.11945\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 0.2930 - recall_2: 0.0000e+00 - recall_3: 1.0000 - precision_2: 0.0000e+00 - precision_3: 1.0000 - val_loss: 2.1874 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.2522 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000        \n",
      "Epoch 19: val_loss improved from 2.11945 to 2.08707, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 444ms/step - loss: 0.2531 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000 - val_loss: 2.0871 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.2298 - recall_2: 0.0000e+00 - recall_3: 0.9231 - precision_2: 0.0000e+00 - precision_3: 1.0000\n",
      "Epoch 20: val_loss did not improve from 2.08707\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.2357 - recall_2: 0.0000e+00 - recall_3: 0.9286 - precision_2: 0.0000e+00 - precision_3: 0.9286 - val_loss: 2.1120 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.1874 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000     \n",
      "Epoch 21: val_loss did not improve from 2.08707\n",
      "6/6 [==============================] - 1s 81ms/step - loss: 0.1874 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000 - val_loss: 2.1039 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1610 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000       \n",
      "Epoch 22: val_loss improved from 2.08707 to 2.07595, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 483ms/step - loss: 0.1713 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000 - val_loss: 2.0760 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 23/30\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.1526 - recall_2: 1.0000 - recall_3: 0.9286 - precision_2: 1.0000 - precision_3: 1.0000        \n",
      "Epoch 23: val_loss did not improve from 2.07595\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.1526 - recall_2: 1.0000 - recall_3: 0.9286 - precision_2: 1.0000 - precision_3: 1.0000 - val_loss: 2.1006 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 24/30\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.1638 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 0.9333      \n",
      "Epoch 24: val_loss improved from 2.07595 to 2.01469, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 426ms/step - loss: 0.1638 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 0.9333 - val_loss: 2.0147 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 25/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1291 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000       \n",
      "Epoch 25: val_loss improved from 2.01469 to 1.96587, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 399ms/step - loss: 0.1386 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000 - val_loss: 1.9659 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 26/30\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.1144 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000     \n",
      "Epoch 26: val_loss improved from 1.96587 to 1.92374, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 3s 580ms/step - loss: 0.1144 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000 - val_loss: 1.9237 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 27/30\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.1346 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000     \n",
      "Epoch 27: val_loss did not improve from 1.92374\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 0.1346 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000 - val_loss: 1.9309 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 28/30\n",
      "6/6 [==============================] - ETA: 0s - loss: 0.1444 - recall_2: 1.0000 - recall_3: 0.9286 - precision_2: 1.0000 - precision_3: 1.0000      \n",
      "Epoch 28: val_loss improved from 1.92374 to 1.88712, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 427ms/step - loss: 0.1444 - recall_2: 1.0000 - recall_3: 0.9286 - precision_2: 1.0000 - precision_3: 1.0000 - val_loss: 1.8871 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 29/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.1363 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000       \n",
      "Epoch 29: val_loss did not improve from 1.88712\n",
      "6/6 [==============================] - 1s 76ms/step - loss: 0.1451 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000 - val_loss: 1.9049 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 30/30\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0842 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000        \n",
      "Epoch 30: val_loss improved from 1.88712 to 1.86748, saving model to ../data/models/conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../data/models/conv/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 465ms/step - loss: 0.0826 - recall_2: 1.0000 - recall_3: 1.0000 - precision_2: 1.0000 - precision_3: 1.0000 - val_loss: 1.8675 - val_recall_2: 0.0000e+00 - val_recall_3: 0.0000e+00 - val_precision_2: 0.0000e+00 - val_precision_3: 0.0000e+00 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15111c2b0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the convolutional neural network\n",
    "conv_classifier.fit(db.train_dataset,\n",
    "                    validation_data=db.val_dataset,\n",
    "                    epochs=n_epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=conv_callbacks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 15:32:25.340513: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [45,18]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 17ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "test_list = list(db.test_dataset.as_numpy_iterator())\n",
    "X_test = [item[0] for item in test_list]\n",
    "y_test = [item[1] for item in test_list]\n",
    "\n",
    "X_test = np.concatenate(X_test, axis=0)\n",
    "y_test = np.concatenate(y_test, axis=0)\n",
    "y_test = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Compute conv metrics\n",
    "y_pred = conv_classifier.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "cm_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
    "cm_display.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "# cm_display.save(\"../figures/confusion_matrix_heatmap.png\", width=6, height=6, dpi=300, facecolor='w')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_model_input(X, y, cell_types):\n",
    "    # Normalize data\n",
    "    X_norm = np.zeros_like(X)\n",
    "    for i in tqdm(range(X.shape[-1])):\n",
    "        X_norm[..., i] = normalize_image(X[..., i])\n",
    "    \n",
    "    # create cell views using bonding boxes\n",
    "    cell_views = np.empty((len(variable_markers.index),), dtype=object)\n",
    "    cell_types_list = np.empty((len(variable_markers.index),), dtype=object)\n",
    "    for i, channel_key in enumerate(tqdm(variable_markers.index)):\n",
    "        cell_views[i], cell_types_list[i] = get_cell_boxes(X_norm[0,:,:,channel_key], y[0,:,:,0], cell_types, use_all_cells=False)\n",
    "    \n",
    "    # format cell views for model input\n",
    "    reshaped_cell_views = np.empty((len(cell_views), len(cell_views[0]), 21, 21))\n",
    "    for i, channel in enumerate(cell_views):\n",
    "        reshaped_cell_views[i] = np.stack(channel)\n",
    "    cell_views_transposed = np.transpose(reshaped_cell_views, (1, 2, 3, 0))\n",
    "\n",
    "    reshaped_cell_types = np.empty((len(cell_types_list), len(cell_types_list[0])))\n",
    "    for i, channel in enumerate(cell_types_list):\n",
    "        reshaped_cell_types[i] = np.stack(channel)\n",
    "    reshaped_cell_types = reshaped_cell_types.transpose((1, 0))\n",
    "\n",
    "    return cell_views_transposed, reshaped_cell_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(X, y, cell_types):\n",
    "    cell_views, cell_types = prep_model_input(X, y, cell_types)\n",
    "    # load pretrained model\n",
    "    model = tf.keras.models.load_model('../data/models/conv')\n",
    "    predictions = model.predict(x_test)\n",
    "    return predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Model execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = run_pipeline(X, y, cell_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bebi205",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
